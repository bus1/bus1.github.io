<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>bus1</title><link rel="stylesheet" type="text/css" href="bus1.css"><meta name="generator" content="DocBook XSL Stylesheets V1.79.1"><link rel="icon" href="bus1.png" type="image/png"><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-75729104-1', 'auto');
      ga('send', 'pageview');
    </script></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><header><a href="index.html"><img src="bus1.svg" alt="bus1" style="width:64px;height:64px;"></a><span class="version">Version 1</span></header><div class="refentry"><a name="bus1"></a><div class="titlepage"></div><div class="refnamediv"><p>bus1 â€” Kernel Message Bus</p></div><div class="refsynopsisdiv"><h2>Synopsis</h2><div class="funcsynopsis"><pre class="funcsynopsisinfo">#include &lt;linux/bus1.h&gt;</pre></div></div><div class="refsect1"><a name="id-1.5"></a><h2>Description</h2><p>
The Bus1 Kernel Message Bus defines and implements a distributed object model.
It allows local processes to send messages to objects owned by remote processes,
as well as share their own objects with others. Object ownership is static and
cannot be transferred. Access to remote objects is prohibited, unless it was
explicitly granted. Processes can transmit messages to a remote object via the
message bus, transferring a data payload, object access rights, file
descriptors, or other auxiliary data.
    </p><p>
To participate on the message bus, a peer context must be created. Peer contexts
are kernel objects, identified by a file descriptor. They are not bound to any
process, but can be shared freely. The peer context provides a message queue to
store all incoming messages, a registry for all locally owned objects, and
tracks access rights to remote objects. A peer context never serves as
routing entity, but merely as anchor for peer-owned resources. Any message on
the bus is always destined at an object, and the bus takes care to transfer a
message into the message queue of the peer context that owns this object.
    </p><p>
The message bus manages object access using capability-based security. That is,
by default only the owner of an object is granted access rights. No other peer
can access the object, nor are they aware of the existance of the object.
However, access rights can be transmitted as auxiliary data with any message,
effectively granting them to the receiver of the message. This even works
transitively, that is, any peer that was granted access to an object can pass on
those rights, even if they do not own the object. But mind that no access rights
can ever be revoked, besides the owner destroying the object.
    </p><div class="refsect2"><a name="id-1.5.5"></a><h3>Nodes and Handles</h3><p>
Each peer context comes with a registry of owned objects, which in bus1
parlance are called <span class="emphasis"><em>nodes</em></span>. A peer is always the exclusive
owner of all nodes it has created. Ownership cannot be transferred. Furthermore, 
initially, the node owner is the only peer with access to a newly created node.
The message bus manages access rights to nodes as a set of
<span class="emphasis"><em>handles</em></span> held by each peer. For each node a peer has access
to, whether it is local or remote, the message bus keeps a handle on the peer.
Those handles are local to each peer, but can be transmitted as auxiliary data
with any message, effectively allocating a new handle to the same node in the
destination peer. This works transitively, and each peer with access rights can
pass them on further, or deliberately drop them again. As long as a peer has
access rights to a node it can send messages to it. However, a node owner can,
at any time, decide to destroy a node. This causes all further message
transactions to this node to fail, and all peers holding access rights to the
node (i.e., they own a handle for that node) are notified of the destruction.
      </p><p>
Handles are the only way to refer to both local and remote nodes. For each
handle allocated on a peer, a 64bit ID is assigned to identify that particular
handle on that particular peer. The ID is only valid locally on that peer. It
cannot be used by remote peers to address the handle (in other words, the ID
namespace is tied to each peer and does not define global entities).
Furthermore, an assigned ID is never reused, even if a handle is dropped. The
kernel keeps a user-reference count for each handle. Every time a handle is
exposed to a peer, the user-reference count of that handle is incremented by
one. This is never done asynchronously, but only synchronously when an ioctl is
called by the holding peer. Therefore, a peer can reliable deduce the current
user-reference count of all its handles, regardless of any ongoing message
transaction. References can be explicitly dropped by a peer. Once the counter of
a handle hits zero, it is destroyed, its ID becomes invalid, and will not be
reused again. Note that a peer can never have multiple different handles to the
same node, rather the kernel always coalesces them into a single handle, using
the user-reference counter to track it. However, if a handle is fully released,
but the peer later acquires a handle to the same node again, its ID will be
different, as IDs are never reused. A concept of soft-references is not
supported.
      </p><p>
When allocating a new node, the node owner implicitly also gets a handle to that
node. As long as the node is valid, the kernel will pin a single user-reference
to the owner's handle. This guarantees that a node owner always retains access
to their node, until they explicitly destroy it (which will also implicitly
release that pinned user-reference on the handle). Otherwise, a handle to a
local node behaves just like any other handle, that is, user-references are
acquired and released according to its use. However, whenever the overall sum of
all user-references on all handles to a node drops to 1 (which implies that only
the pinned reference of the owner is left), a notification is queued on the node
owner. If the counter is incremented again, any such notification is dropped, if
not already dequeued.
      </p></div><div class="refsect2"><a name="id-1.5.6"></a><h3>Message Transactions</h3><p>

To be defined.
      </p></div><div class="refsect2"><a name="id-1.5.7"></a><h3>Global Ordering</h3><p>
Despite there being no global synchronization, all events on the bus, such as
sending or receiving of messages, release of handles or destruction of nodes,
behave as if they were globally ordered. That is, for any two events it is
always possible to consider one to have happened before the other in such a way
that it is consistent with all the effects observed on the bus.
      </p><p>
For instance, if two events occurr on one peer (say the sending of a message,
and the destruction of a node), and they are observed on another peer (by
receiving the message and receiving a destruction notification for the node), we
are guaranteed that the order the events occurred in and the order they were
observed in is the same.
      </p><p>
One could consider a further example involving three peers, if a message is sent
from one peer to two others, and after receiving the message the first recipient
sends a further message to the second recipient, it is guaranteed that the
original message is received before the subsequent one.
      </p><p>
This principle of causality is also respected in the pressence of side-channel
communication. That is, if one event may have triggered another, even if on
different, disconnected, peers, we are guaranteed that the events are ordered
accordingly. To be precise, if one event (such as receiving a message) completed
before another (such as sending a message) was started, then they are ordered
accordingly.
      </p><p>
Also in the case where there can be no causal relationship, are we guaranteed
a global order. In case two events happend concurrently, there can never be any
inconsistency in which occurred before the other. By way of example, consider
two peers sending one message each to two different peers, we are guaranteed
that both the recipient peers receive the two messages in the same order, even
though the order may be arbitrary.
      </p></div><div class="refsect2"><a name="id-1.5.8"></a><h3>Operating on a bus1 file descriptor</h3><p>
The bus1 peer file descriptor supports the following operations:
      </p><div class="variablelist"><dl class="variablelist"><dt><span class="term">
            <a href="http://linux.die.net/man/2/open"><span class="citerefentry"><span class="refentrytitle">open</span>(2)</span></a>
          </span></dt><dd><p>
A call to
<a href="http://linux.die.net/man/2/open"><span class="citerefentry"><span class="refentrytitle">open</span>(2)</span></a>
on the bus1 character device (usually <code class="filename">/dev/bus1</code>) creates a
new peer context identifie by the returned file descriptor.
            </p></dd><dt><span class="term">
            <a href="http://linux.die.net/man/2/poll"><span class="citerefentry"><span class="refentrytitle">poll</span>(2)</span></a>
          , </span><span class="term">
            <a href="http://linux.die.net/man/2/select"><span class="citerefentry"><span class="refentrytitle">select</span>(2)</span></a>
          , </span><span class="term">(and similar)</span></dt><dd><p>
The file descriptor supports
<a href="http://linux.die.net/man/2/poll"><span class="citerefentry"><span class="refentrytitle">poll</span>(2)</span></a>
(and analogously
<a href="http://linux.die.net/man/7/epoll"><span class="citerefentry"><span class="refentrytitle">epoll</span>(7)</span></a>) and
<a href="http://linux.die.net/man/2/select"><span class="citerefentry"><span class="refentrytitle">select</span>(2)</span></a>, as follows:
            </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
The file descriptor is readable (the <code class="varname">readfds</code> argument of
<a href="http://linux.die.net/man/2/select"><span class="citerefentry"><span class="refentrytitle">select</span>(2)</span></a>;
the <code class="constant">POLLIN</code> flag of
<a href="http://linux.die.net/man/2/poll"><span class="citerefentry"><span class="refentrytitle">poll</span>(2)</span></a>)
if one or more messages are ready to be dequeued.
                </p></li><li class="listitem"><p>
The file descriptor is writable (the <code class="varname">writefds</code> argument of
<a href="http://linux.die.net/man/2/select"><span class="citerefentry"><span class="refentrytitle">select</span>(2)</span></a>;
the <code class="constant">POLLOUT</code> flag of
<a href="http://linux.die.net/man/2/poll"><span class="citerefentry"><span class="refentrytitle">poll</span>(2)</span></a>)
if the peer has not been shut down, yet (i.e., the peer can be used to send
messages).
                </p></li><li class="listitem"><p>
The file descriptor signals a hang-up (overloaded on the
<code class="varname">readfds</code> argument of
<a href="http://linux.die.net/man/2/select"><span class="citerefentry"><span class="refentrytitle">select</span>(2)</span></a>;
the <code class="constant">POLLHUP</code> flag of
<a href="http://linux.die.net/man/2/poll"><span class="citerefentry"><span class="refentrytitle">poll</span>(2)</span></a>)
if the peer has been shut down.
                </p></li></ul></div><p>
The bus1 peer file descriptor also supports the other file descriptor
multiplexing APIs:
<a href="http://linux.die.net/man/2/pselect"><span class="citerefentry"><span class="refentrytitle">pselect</span>(2)</span></a>, and
<a href="http://linux.die.net/man/2/ppoll"><span class="citerefentry"><span class="refentrytitle">ppoll</span>(2)</span></a>.
            </p></dd><dt><span class="term">
            <a href="http://linux.die.net/man/2/mmap"><span class="citerefentry"><span class="refentrytitle">mmap</span>(2)</span></a>
          </span></dt><dd><p>
A call to
<a href="http://linux.die.net/man/2/mmap"><span class="citerefentry"><span class="refentrytitle">mmap</span>(2)</span></a>
installs a memory mapping to the message pool of the peer into the caller's
address-space. No writable mappings are allowed. Furthermore, the pool has no
fixed size, but grows dynamically with the demands of the peer.
            </p></dd><dt><span class="term">
            <a href="http://linux.die.net/man/2/ioctl"><span class="citerefentry"><span class="refentrytitle">ioctl</span>(2)</span></a>
          </span></dt><dd><p>
The following bus1-specific commands are supported:
            </p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="constant">BUS1_CMD_PEER_RESET</code></span></dt><dd><p>
This command resets a peer context to its initial state. It takes a 64-bit flags
argument. If no flags are passed, all nodes owned by this peer context are
destroyed atomically, followed by a release of all held handles. Lastly, all
remaining slices are flushed from the pool. Only the node destruction is atomic.
The remaining cleanup is not. This means, if another command is executed in
parallel, it might be partially affected by the ongoing reset.
                  </p><p>
Any node marked as persistent, as well as the seed message, are preserved and
will survive a reset operation.
                  </p><p>
If the <code class="constant">BUS1_RESET_FLAG_DISCONNECT</code> is set, no new
operations can be performed on the peer context and all resources, including
persistent nodes, are released.
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_HANDLE_TRANSFER</code></span></dt><dd><p>
This command transfers a handle from one peer context to another. It takes the
following structure as argument:
</p><pre class="programlisting">
struct bus1_cmd_handle_transfer {
        __u64 flags;
        __u64 src_handle;
        __u64 dst_fd;
        __u64 dst_handle;
};
</pre><p>
<code class="varname">flags</code> must always be set to 0, <code class="varname">src_handle</code>
is the handle ID of the handle being transferred in the source context,
<code class="varname">dst_fd</code> is the file descriptor representing the destination
peer context and <code class="varname">dst_handle</code> must be
<code class="constant">BUS1_HANDLE_INVALID</code> and is set to the new handle ID in the
destination context on return.
		</p><p>
If the <code class="constant">BUS1_NODE_FLAG_ALLOCATE</code> flag is set in
<code class="varname">src_handle</code> a new node is allocated in the source peer context
and <code class="varname">src_handle</code> is set to its handle ID on return. If
<code class="varname">dst_fd</code> is set to <code class="constant">-1</code> the source context
is also used as the destination.
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_HANDLE_RELEASE</code></span></dt><dd><p>
This command releases one user reference to a handle. It takes a handle ID as
argument.
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_NODE_DESTROY</code></span></dt><dd><p>
This command destroys a node. It takes a handle ID as argument. The referenced
handle must be the owner handle of a local node.
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_SLICE_RELEASE</code></span></dt><dd><p>
This command releases one slice from the local pool. It takes a pool offset to
the start of the slice to be released.
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_SEND</code></span></dt><dd><p>
This command sends a message. It takes the following structure as argument:
</p><pre class="programlisting">
struct bus1_cmd_send {
        __u64 flags;
        __u64 ptr_destinations;
        __u64 n_destinations;
        __u64 ptr_vecs;
        __u64 n_vecs;
        __u64 ptr_handles;
        __u64 n_handles;
        __u64 ptr_fds;
        __u64 n_fds;
};
</pre><p>
                  </p><p>
<code class="varname">flags</code> may be set to at most one of
<code class="constant">BUS1_SEND_FLAG_CONTINUE</code> and
<code class="constant">BUS1_SEND_FLAG_SEED</code>. If
<code class="constant">BUS1_SEND_FLAG_CONTINUE</code> is set any messages that cannot
be delivered due to errors on the remote peer are silently dropped and the
error is instead reported to the receiver, otherwise such an error means the
whole transaction fails. If <code class="constant">BUS1_SEND_FLAG_SEED</code> is set the
message replaces the seed message on the local peer. In this case,
<code class="varname">n_destinations</code> must be 0.
                  </p><p>
<code class="varname">ptr_destinations</code> is a pointer to an array of handle IDs and
<code class="varname">n_destinations</code> is the length of the array. If the
<code class="constant">BUS1_NODE_FLAG_ALLOCATE</code> is set in either of the handle IDs
a new node is allocated in the local peer context, and the handle ID is replaced
by the ID of the new node on return. The message being sent is delivered to
the peer context owning the nodes pointed to by each of the handles in the
array.
                  </p><p>
<code class="varname">ptr_vecs</code> is a pointer to an array of iovecs and
<code class="varname">n_vecs</code> is the length of the array. The iovecs represent the
payload of the message which is delivered to each destination.
                  </p><p>
<code class="varname">ptr_handles</code> is a pointer to an array of handle IDs and
<code class="varname">n_handles</code> is the length of the array. Nodes may be allocated
on-demand as described above. Each of the handles in this array is installed
in each destination peer context at receive time.
                  </p><p>
<code class="varname">ptr_fds</code> is a pointer to an integer array of file descriptors
and <code class="varname">n_fds</code> is the length of the array. Each of the file
descriptors in this array may be installed in the destination peer context at
receive time (see below).
                  </p></dd><dt><span class="term"><code class="constant">BUS1_CMD_RECV</code></span></dt><dd><p>
This command receives a message. It takes the following structure as argument:
</p><pre class="programlisting">
struct bus1_cmd_recv {
        __u64 flags;
        __u64 n_dropped;
        struct {
                __u64 type;
                __u64 destination;
                __u32 uid;
                __u32 gid;
                __u32 pid;
                __u32 tid;
                __u64 offset;
                __u64 n_bytes;
                __u64 n_handles;
                __u64 n_fds;
        } msg;
};
</pre><p>
If <code class="constant">BUS1_RECV_FLAG_PEEK</code> is set in <code class="varname">flags</code>,
the received message is not dropped from the queue. If
<code class="constant">BUS1_RECV_FLAG_SEED</code> is set, the peer's seed is received
rather than a message from the queue. If
<code class="constant">BUS1_RECV_FLAG_INSTALL_FDS</code> the file descriptors attached to
the received message are installed in the receiving process. Care must be taken
when using the last flag from more than one process on the same message as file
descriptor numbers are per-process and not per-peer.
                  </p><p>
<code class="varname">n_dropped</code> indicates the number of silently dropped messages
destined for this peer. The count is reset unless the
<code class="constant">BUS1_RECV_FLAG_PEEK</code> flag was passed.
                  </p><p>
<code class="varname">msg.type</code> indicates the type of message.
<code class="constant">BUS1_MSG_NONE</code> is used to only receive
<code class="varname">n_dropped</code>. <code class="constant">BUS1_MSG_DATA</code> indicates a
regular message sent from another peer, possibly containing a payload, as well
as attached handles and filedescriptors.
<code class="constant">BUS1_MSG_NODE_DESTROY</code> indicates that the node referenced
by the handle in <code class="varname">msg.destination</code> was destroyed by its owner.
<code class="constant">BUS1_MSG_NODE_RELEASE</code> indicates that all the references to
handles referencing the node in <code class="varname">msg.destination</code> have been
released.
                  </p><p>
<code class="varname">msg.destination</code> is the ID of the destination node or handle
of the message.
                  </p><p>
<code class="varname">msg.uid</code>, <code class="varname">msg.gid</code>,
<code class="varname">msg.pid</code>, and <code class="varname">msg.tid</code> are the user, group,
process and thread ID of the process that created the sending peer context.
                  </p><p>
<code class="varname">msg.offset</code> is the offset, in bytes, into the pool of the
payload and <code class="varname">msg.n_bytes</code> is its length.
                  </p><p>
<code class="varname">msg.n_handles</code> is the number of handles attached to the
message. The handle IDs are stored in the pool following the payload (and
possibly padding to make the array 8-byte aligned).
                  </p><p>
<code class="varname">msg.n_fds</code> is the number of handles attached to the
message, or 0 if <code class="constant">BUS1_RECV_FLAG_INSTALL_FDS</code> was not set.
The file descriptor numbers are stored in the pool following the handle array
(and possibly padding to make the array 8-byte aligned).
                  </p></dd></dl></div></dd><dt><span class="term">
            <a href="http://linux.die.net/man/2/close"><span class="citerefentry"><span class="refentrytitle">close</span>(2)</span></a>
          </span></dt><dd><p>
A call to
<a href="http://linux.die.net/man/2/close"><span class="citerefentry"><span class="refentrytitle">close</span>(2)</span></a>
releases the passed file descriptor. When all file descriptors associated with
the same peer context have been closed, the peer is shut down. This destroys all
nodes of that peer, releases all handles, flushes its queue and pool, and
deallocates all related resources. Messages that have been sent by the peer and
are still queued on destination queues, are unaffected by this.
            </p></dd></dl></div></div></div><div class="refsect1"><a name="id-1.6"></a><h2>Return value</h2><p>
All bus1 operations return zero on success. On failure, a negative error code is
returned.
    </p></div><div class="refsect1"><a name="id-1.7"></a><h2>Errors</h2><p>
These are all standard errors generated by the bus layer. See the description
of each ioctl for details on their occurrence.
    </p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="constant">EAGAIN</code></span></dt><dd><p>
No messages ready to be read.
        </p></dd><dt><span class="term"><code class="constant">EBADF</code></span></dt><dd><p>
Invalid file descriptor.
        </p></dd><dt><span class="term"><code class="constant">EDQUOT</code></span></dt><dd><p>
Resource quota exceeded.
        </p></dd><dt><span class="term"><code class="constant">EFAULT</code></span></dt><dd><p>
Cannot read, or write, ioctl parameters.
        </p></dd><dt><span class="term"><code class="constant">EHOSTUNREACH</code></span></dt><dd><p>
The destination object is no longer available.
        </p></dd><dt><span class="term"><code class="constant">EINVAL</code></span></dt><dd><p>
Invalid ioctl parameters.
        </p></dd><dt><span class="term"><code class="constant">EMSGSIZE</code></span></dt><dd><p>
The message to be sent exceeds its allowed resource limits.
        </p></dd><dt><span class="term"><code class="constant">ENOMEM</code></span></dt><dd><p>
Out of kernel memory.
        </p></dd><dt><span class="term"><code class="constant">ENOTTY</code></span></dt><dd><p>
Unknown ioctl.
        </p></dd><dt><span class="term"><code class="constant">ENXIO</code></span></dt><dd><p>
Unknown object.
        </p></dd><dt><span class="term"><code class="constant">EOPNOTSUPP</code></span></dt><dd><p>
Operation not supported.
        </p></dd><dt><span class="term"><code class="constant">EPERM</code></span></dt><dd><p>
Permission denied.
        </p></dd><dt><span class="term"><code class="constant">ESHUTDOWN</code></span></dt><dd><p>
Local peer was already shut down.
        </p></dd></dl></div></div><div class="refsect1"><a name="id-1.8"></a><h2>See Also</h2><span class="simplelist">
        <a href="bus1.peer.html"><span class="citerefentry"><span class="refentrytitle">bus1.peer</span>(7)</span></a>
      , 
        <a href="bus1.node.html"><span class="citerefentry"><span class="refentrytitle">bus1.node</span>(7)</span></a>
      , 
        <a href="bus1.message.html"><span class="citerefentry"><span class="refentrytitle">bus1.message</span>(7)</span></a>
      , 
        <a href="bus1.pool.html"><span class="citerefentry"><span class="refentrytitle">bus1.pool</span>(7)</span></a>
      </span></div></div></body></html>
